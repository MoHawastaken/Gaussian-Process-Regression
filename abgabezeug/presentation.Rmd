---
title: "Gaussian Processes"
author: "Budjan, Haas, Klumpp, Reitze"
date: "22. Februar 2019"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(gprc)
#Copy function definitions, since they aren't being exported by the package
multivariate_normal <- function(n, mean, covariance) {
  stopifnot(is.numeric(mean), is.numeric(covariance), length(mean) == nrow(covariance))
  L <- t(chol(covariance))
  c(mean) + L %*% matrix(rnorm(n*length(mean), 0, 1), nrow = length(mean))
}
sqrexp <- function(x, y, l) UseMethod("sqrexp")
sqrexp.matrix <- function(x, y, l) exp(-colSums((x - y)^2)/(2 * l^2))
sqrexp.numeric <- function(x, y, l) exp(-sum((x - y)^2)/(2 * l^2))
k <- 50 #DPI Anzahl, setze niedrig, um schneller zu rendern
```

## Regression und Klassifikation
```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
n <- 10
X <- seq(-5, 5, by = 1)
y <- sin(X)
kappa <- function(x,y) sqrexp(x,y,l=1)
ggplot2::ggplot(data = data.frame(x = X, y = y), ggplot2::aes(x = x, y = y)) + ggplot2::theme_classic() + ggplot2::geom_point()
```

## Regression und Klassifikation

```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
n <- 10
X <- cbind(multivariate_normal(n, c(0.5,0.5), diag(c(0.1,0.1))), multivariate_normal(n, c(-0.5,-0.5), diag(c(0.1,0.1))))
y <- rep(c(1,-1), each = n)
ggplot2::ggplot(data = data.frame(x = X[1, ], y = X[2, ], label = y), ggplot2::aes(x = x, y = y, shape = factor(label))) + ggplot2::theme_classic() +
 ggplot2::scale_y_continuous(expression(x[2])) +
 ggplot2::scale_x_continuous(expression(x[1])) +ggplot2::geom_point() + 
  ggplot2::scale_shape_manual(values = c(4, 2)) + 
  ggplot2::guides(shape = ggplot2::guide_legend(title = "Testpoints"))
```

## Einführung zu Gaußschen Prozessen

Ein Gaußscher Prozess ist ein stochastischer Prozess $(f_x)_{x \in X}$ wobei für jede endliche Teilmenge $Y \subset X$, $(f_x)_{x \in Y}$ multivariat normal verteilt ist.  
$f(x) := f_x$, $(f_x)_{x \in X}$ wird aufgefasst als zufällige Funktion $f:X \to \mathbb{R}$.

## Anwendung

- Regression
- Klassifikation

## Mathe zu GPR

Mit vorgegebenen Datenpunkten $(X, f(X))$ und Testpunkten $X_*$ ist die gemeinsame Verteilung gegeben durch
$$
\begin{pmatrix}
f(X) \\
f(X_*)
\end{pmatrix}
 \sim \mathcal{N} \left( 0, 
 \begin{pmatrix}
 	K & \mathbf{k_*} \\
 	\mathbf{k_*} & k(x_*, x_*)
 \end{pmatrix}
 \right)
$$
Die vorhergesagten Werte der Testpunkte erhalten wir aus der bedingten Verteilung
$$
f(X_*)|f(X), K, \mathbf{k_*}, k(x_*, x_*)
$$

durch ihren Erwartungswert
$$
  \mathbf{k_*} K^{-1} f(X)
$$

## GPR
`GPR$new(X, y, noise, cov_func)`

- R6 Klasse
- speichert Daten, vorberechnete Matrizen
- Methoden `$predict`, `$plot`, `$plot_posterior_draws`, `$plot_posterior_variance`

## predict Algorithmus

Zusätzliche Annahme: noisy data  
Daten werden mit Fehler $\varepsilon$ beobachtet, $y_i = f(x_i) + \varepsilon_i$
$\varepsilon_i \overset{\text{iid}}{\sim} \mathcal{N}(0, \sigma_n^2)$

## predict Algorithmus

Inputs: X (inputs), y (targets), $\sigma_n^2$ (noise), k(covariance funtion), $X_*$ (test input)

1. $L=\text{cholesky}(K(X,X)+\sigma_n^2 I)$
2. $\alpha=\text{solve}(L^T,\text{solve}(L,y))$
3. $\bar{f}(X_*)=K(X,X_*)^T\cdot\alpha$
4. $v=\text{solve}(L,K(X,X_*))$
5. $\bar{\text{V}}(\bar{f}(X_*))=k(X_*,X_*)-v^Tv$

return: $\bar{f}(X_*)$, $\bar{\text{V}}(f(X_*))$


## predict Algorithmus
<img src="https://trello-attachments.s3.amazonaws.com/5c1367e7bda778491a4a4fde/5c1b68f238a2808027971811/dd63ca23dffa380ebbbce68d2d931ae2/algorithm_2_1_clear.png"
     alt="algorithm1"
     style="width:800px;" />

## Regression

```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62}
X <- matrix(seq(-5,5,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
Gaussian <- GPR.sqrexp$new(X, y, noise, 1)
Gaussian$plot(seq(-6,6, by = 0.05))$plot
```


## Optimierung der Hyperparameter

- Maximierung der Marginal Log-Likelihood
- Newton Methode, `optim()`

## fit()

`fit(X, y, noise, cov_names)`

- Funktioniert nur über eingespeicherte Kovarianzfunktionen 
- Findet für jede Kovarianzfunktion in `cov_names` die optimalen Parameter
- Gibt die beste Kovarianzfunktion mit optimalen Parametern zurück
- error handling bei nicht invertierbaren / nicht positiv definiten Matrizen

## Gewählte Kovarianzfunktion

```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62}
X <- matrix(seq(-5,5,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
Gaussian <- GPR.sqrexp$new(X, y, noise, 1)
Gaussian$plot(seq(-6,6, by = 0.05))$plot
```

## Optimierte Kovarianzfunktion

```{r dpi=k, out.width="95%", echo=FALSE, fig.width = 5, fig.asp = .62}
X <- matrix(seq(-5,5,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
z <- fit(X,y,noise,list("linear","constant","polynomial","sqrexp","gammaexp","rationalquadratic"))
Gaussian <- GPR$new(X, y, noise, z$func)
Gaussian$plot(seq(-6,6, by = 0.05))$plot
message(sprintf("Optimal covariance function: %s",z$cov))
```

## Mathe zu GPC

## GPC

- R6 Klasse
- Methoden `$predict`, `$plot`

## Simulation

## Simulation Beispiele

```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=FALSE}
set.seed(0)
# Haus
  f <- function(x) {
    ((sum(abs(x)) < 4 && x[2] > 1)|| (x[1] > -3 && x[1] < 3 && x[2] <= 1)) - 
      (!((sum(abs(x)) < 4 && x[2] > 1)|| (x[1] > -3 && x[1] < 3 && x[2] <= 1))) -
      2*(x[1] > -0.75 && x[1] < 0.75 && x[2] < -2) -
      2*(x[1] > -2 && x[1] < -0.5 && x[2] > -1 && x[2] < 0.5) -
      2*(x[1] > 0.5 && x[1] < 2 && x[2] > -1 && x[2] < 0.5)
  }
  limits <- matrix(c(-4, 4, -4, 4), nrow = 2, byrow = TRUE)
  k <- function(x, y) sqrexp(x, y, 1)
  simulate_classification(func = f, limits = limits, k = k, num_data = 600)
```

## Simulation Beispiele

```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=FALSE}
set.seed(0)
 # Example R
  g <- function(x){
    if (x == 0) return(-1)
    if (x == -3) return(-1)
    return(x)
  }
  f <- function(x) {
    g(((x[1] > -3.2 && x[1] < 1.8 && x[2] <= 3.2 && x[2] > 0) || (x[1] > -3 && x[1] < 3.5 && x[2] <= 3 && x[2] <= 0) ) - 
        (!((x[1] > -3.2 && x[1] < 1.8 && x[2] <= 3.2 && x[2] > 0) || (x[1] > -3 && x[1] < 3.5 && x[2] <= 3 && x[2] <= 0) )) -
        2*(x[1] > -1.5 && x[1] < 0 && x[2] < -1) -
        2*(x[1] > 0 && x[1] < 2 && x[2]  < -1 && sum(x) < -1.8) -
        2*(x[1] > -2 && x[1] < 0.5 && x[2] > 0 && x[2] < 2) -
        2*(x[1] > 1 && x[2] < 0 && x[2] > -3.5 && x[1] < 3.5 && sum(x) > -0.4))
  }
  
  limits <- matrix(c(-4, 4, -4, 4), nrow = 2, byrow = TRUE)
  k <- function(x, y) sqrexp(x, y, 1)
  simulate_classification(func = f, limits = limits, k = k, num_data = 600)
```

## Shiny App
