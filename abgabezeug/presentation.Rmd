---
title: "Gaussian Process Presentation"
author: "Budjan, Haas, Klumpp, Reitze"
date: "22 Februar 2019"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(gprc)
#Copy function definitions, since they aren't being exported by the package
multivariate_normal <- function(n, mean, covariance) {
  stopifnot(is.numeric(mean), is.numeric(covariance), length(mean) == nrow(covariance))
  L <- t(chol(covariance))
  c(mean) + L %*% matrix(rnorm(n*length(mean), 0, 1), nrow = length(mean))
}
sqrexp <- function(x, y, l) UseMethod("sqrexp")
sqrexp.matrix <- function(x, y, l) exp(-colSums((x - y)^2)/(2 * l^2))
sqrexp.numeric <- function(x, y, l) exp(-sum((x - y)^2)/(2 * l^2))
```

## Regression und Klassifikation
```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
n <- 10
X <- seq(-5, 5, by = 1)
y <- sin(X)
kappa <- function(x,y) sqrexp(x,y,l=1)
ggplot2::ggplot(data = data.frame(x = X, y = y), ggplot2::aes(x = x, y = y)) + ggplot2::theme_classic() + ggplot2::geom_point()
```

## Regression und Klassifikation

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
n <- 10
X <- cbind(multivariate_normal(n, c(0.5,0.5), diag(c(0.1,0.1))), multivariate_normal(n, c(-0.5,-0.5), diag(c(0.1,0.1))))
y <- rep(c(1,-1), each = n)
ggplot2::ggplot(data = data.frame(x = X[1, ], y = X[2, ], label = y), ggplot2::aes(x = x, y = y, shape = factor(label))) + ggplot2::theme_classic() +
 ggplot2::scale_y_continuous(expression("x_2")) +
 ggplot2::scale_x_continuous(expression("x_1")) +ggplot2::geom_point() + 
  ggplot2::scale_shape_manual(values = c(4, 2)) + 
  ggplot2::guides(shape = ggplot2::guide_legend(title = "Testpoints"))
```

## Einführung zu Gaußschen Prozessen

## Anwendung

- Regression
- Klassifikation

## Mathe zu GPR

## GPR
`GPR$new(X, y, noise, cov_func)`

- R6 Klasse
- Methoden `$predict`, `$plot`

## predict Algorithmus
<img src="https://trello-attachments.s3.amazonaws.com/5c1367e7bda778491a4a4fde/5c1b68f238a2808027971811/dd63ca23dffa380ebbbce68d2d931ae2/algorithm_2_1_clear.png"
     alt="algorithm1"
     style="width:800px;" />

## Regression

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62}
X <- matrix(seq(-5,5,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
Gaussian <- GPR.sqrexp$new(X, y, noise, 1)
Gaussian$plot(seq(-6,6, by = 0.05))$plot
```


## Optimierung der Hyperparameter

- Maximierung der Marginal log likelihood
- Newton Methode, `optim()`

## fit()

`fit(X, y, noise, cov_names)`

- Funktioniert nur über eingespeicherte Kovarianzfunktionen 
- Findet für jede Kovarianzfunktion in `cov_names` die optimalen Parameter
- Gibt die optimale Kovarianzfunktion mit besten Parametern zurück

## Gewählte Kovarianzfunktion

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62}
X <- matrix(seq(-5,5,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
Gaussian <- GPR.sqrexp$new(X, y, noise, 1)
Gaussian$plot(seq(-6,6, by = 0.05))$plot
```

## Optimierte Kovarianzfunktion

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62}
X <- matrix(seq(-5,5,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
Gaussian <- GPR$new(X, y, noise)
Gaussian$plot(seq(-6,6, by = 0.05))$plot
```

## Mathe zu GPC

## GPC

- R6 Klasse
- Methoden `$predict`, `$plot`

## Simulation

## Simulation Beispiele

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=FALSE}
set.seed(0)
# Haus
  f <- function(x) {
    ((sum(abs(x)) < 4 && x[2] > 1)|| (x[1] > -3 && x[1] < 3 && x[2] <= 1)) - 
      (!((sum(abs(x)) < 4 && x[2] > 1)|| (x[1] > -3 && x[1] < 3 && x[2] <= 1))) -
      2*(x[1] > -0.75 && x[1] < 0.75 && x[2] < -2) -
      2*(x[1] > -2 && x[1] < -0.5 && x[2] > -1 && x[2] < 0.5) -
      2*(x[1] > 0.5 && x[1] < 2 && x[2] > -1 && x[2] < 0.5)
  }
  limits <- matrix(c(-4, 4, -4, 4), nrow = 2, byrow = TRUE)
  k <- function(x, y) sqrexp(x, y, 1)
  simulate_classification(func = f, limits = limits, k = k, num_data = 600)
```

## Simulation Beispiele

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=FALSE}
set.seed(0)
 # Example R
  g <- function(x){
    if (x == 0) return(-1)
    if (x == -3) return(-1)
    return(x)
  }
  f <- function(x) {
    g(((x[1] > -3.2 && x[1] < 1.8 && x[2] <= 3.2 && x[2] > 0) || (x[1] > -3 && x[1] < 3.5 && x[2] <= 3 && x[2] <= 0) ) - 
        (!((x[1] > -3.2 && x[1] < 1.8 && x[2] <= 3.2 && x[2] > 0) || (x[1] > -3 && x[1] < 3.5 && x[2] <= 3 && x[2] <= 0) )) -
        2*(x[1] > -1.5 && x[1] < 0 && x[2] < -1) -
        2*(x[1] > 0 && x[1] < 2 && x[2]  < -1 && sum(x) < -1.8) -
        2*(x[1] > -2 && x[1] < 0.5 && x[2] > 0 && x[2] < 2) -
        2*(x[1] > 1 && x[2] < 0 && x[2] > -3.5 && x[1] < 3.5 && sum(x) > -0.4))
  }
  
  limits <- matrix(c(-4, 4, -4, 4), nrow = 2, byrow = TRUE)
  k <- function(x, y) sqrexp(x, y, 1)
  simulate_classification(func = f, limits = limits, k = k, num_data = 600)
```

## Shiny App

## Incremental bullets
>- Use this format
>- to have bullets appear
>- one at a time (incrementally)