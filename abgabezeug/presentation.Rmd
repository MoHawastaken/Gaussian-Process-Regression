---
title: "Gaussian Processes"
author: "Budjan, Haas, Klumpp, Reitze"
date: "Datum: 22. Februar 2019"
output: beamer_presentation
header-includes:
   - \usepackage{color}
   - \usepackage{soul}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(gprc)
#Copy function definitions, since they aren't being exported by the package
covariance_matrix <- function(A, B, covariance_function) {
  outer(1:ncol(A), 1:ncol(B), function(i, j) covariance_function(A[, i, drop = F], B[, j, drop = F]))
}
multivariate_normal <- function(n, mean, covariance, tol = 1e-6) {
  stopifnot(length(mean) == nrow(covariance))
  L <- tryCatch(error = function(cond) return(NULL), t(chol(covariance)))
  if (is.null(L)) {
    eig <- eigen(covariance, symmetric = TRUE)
    eigval <- eig$values
    stopifnot(all(eigval > -tol * abs(eigval[1])))
    L <- eig$vectors %*% diag(sqrt(pmax(eigval,0)))
  }
  drop(mean) + L %*% matrix(rnorm(n*length(mean), 0, 1), nrow = length(mean))
}
sqrexp <- function(x, y, l) UseMethod("sqrexp")
sqrexp.matrix <- function(x, y, l) exp(-colSums((x - y)^2)/(2 * l^2))
sqrexp.numeric <- function(x, y, l) exp(-sum((x - y)^2)/(2 * l^2))
k <- 50 #DPI Anzahl, setze niedrig, um schneller zu rendern
```

## Regression und Klassifikation
```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
set.seed(0)
X <- seq(-8, 8, by = 0.4)
y <- (X^3 + 2*sin(3*X))*exp(-abs(X)) + rnorm(length(X), sd = 0.1)
ggplot2::ggplot(data = data.frame(x = X, y = y), ggplot2::aes(x = x, y = y, shape = 4)) +
                   ggplot2::scale_shape_identity() + ggplot2::theme_classic() + ggplot2::geom_point() + 
        ggplot2::coord_cartesian(ylim = c(-2.2, 2.2), xlim = c(-10, 10))
```

## Regression und Klassifikation

```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
set.seed(0)
g <- function(x){
    if (x == 0) return(-1)
    if (x == -3) return(-1)
    return(x)
  }
  f <- function(x) {
    g(((x[1] > -3.2 && x[1] < 1.8 && x[2] <= 3.2 && x[2] > 0) || (x[1] > -3 && x[1] < 3.5 && x[2] <= 3 && x[2] <= 0) ) - 
        (!((x[1] > -3.2 && x[1] < 1.8 && x[2] <= 3.2 && x[2] > 0) || (x[1] > -3 && x[1] < 3.5 && x[2] <= 3 && x[2] <= 0) )) -
        2*(x[1] > -1.5 && x[1] < 0 && x[2] < -1) -
        2*(x[1] > 0 && x[1] < 2 && x[2]  < -1 && sum(x) < -1.8) -
        2*(x[1] > -2 && x[1] < 0.5 && x[2] > 0 && x[2] < 2) -
        2*(x[1] > 1 && x[2] < 0 && x[2] > -3.5 && x[1] < 3.5 && sum(x) > -0.4))
  }
  
  limits <- matrix(c(-4, 4, -4, 4), nrow = 2, byrow = TRUE)
  training_size <- 600
  training_points <- t(sapply(1:nrow(limits), 
                        function(i) runif(training_size, limits[i, 1], limits[i, 2])))
  y <- apply(training_points, 2, f)
ggplot2::ggplot(data = data.frame(x = training_points[1, ], y = training_points[2, ], label = y), ggplot2::aes(x = x, y = y, shape = factor(label))) + ggplot2::theme_classic() +
 ggplot2::scale_y_continuous(expression(x[2])) +
 ggplot2::scale_x_continuous(expression(x[1])) + ggplot2::geom_point() + 
  ggplot2::scale_shape_manual(values = c(4, 2)) + 
  ggplot2::guides(shape = ggplot2::guide_legend(title = "Testpoints"))
```

## Einführung zu Gaußschen Prozessen

>- Stochastischer Prozess: $f:\Omega \times X \rightarrow Z$, $(\omega,x) \mapsto f_x(\omega)$ wobei $f_x(\omega)$ messbar in $\omega$ für alle $x\in X$.     

>- Anschaulich: Stochastischer Prozess ist Zufallsvariable mit Werten $f_x$ in Funktionenraum

>- Ein Gaußscher Prozess ist ein stochastischer Prozess $(f_x)_{x \in X}$ wobei für jede endliche Teilmenge $Y \subset X$, $(f_x)_{x \in Y}$ multivariat normal verteilt ist.  

>- Wird charakterisiert durch $m(x)=\mathbb{E}({f(x)})$ und $k(x,x')=\text{Cov}(f(x),f(x'))$

## Einführung zu Gaußschen Prozessen
Im Folgenden: $X = \mathbb{R}^d$ und $m(x) = 0$  
Beispiel für k: $k(x,y) = \exp \left(-\frac{(x-y)^2}{2\ell}\right)$
3 Beobachtungen eines Gausschen Prozess' mit dieser Kovarianzfunktion (l = 1)

```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
set.seed(0)
X <- matrix(seq(-10, 10, length.out = 1000), nrow = 1)
K <- covariance_matrix(X, X, cov_func(sqrexp, l = 2))
f <- multivariate_normal(3, rep(0, nrow(K)), K)
dat <- data.frame(x = drop(X), f = f)
dat1 <- tidyr::gather(dat, -x, key = "observation", value = "value")
ggplot2::ggplot(dat1, ggplot2::aes(x = x, y = value, colour = observation)) +
  ggplot2::theme_classic() +
  ggplot2::scale_y_continuous("Random functions drawn from GP") +
  ggplot2::geom_line()
```

## Einführung zu Gaußschen Prozessen
3 Beobachtungen eines Gausschen Prozess' mit dieser Kovarianzfunktion (l = 1),
bedingt auf die Datenpunkte (-5,0), (0,3), (7, -3)

```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
set.seed(0)
Gaussian <- GPR.sqrexp$new(c(-5, 0, 7), c(0, 3, -3), noise = 0, l = 1)
Gaussian$plot_posterior_draws(3, c(-10, 10), 1000L)
```

## Theorie zu Regression

Gemeinsame Verteilung für Datenpunkte $(X, f(X))$ und Testpunkte:
$$
\begin{pmatrix}
f(X) \\
f(X_*)
\end{pmatrix}
 \sim \mathcal{N} \left( 0, 
 \begin{pmatrix}
 	K(X,X) & K(X, X_*) \\
 	K(X_*, X) & K(X_*, X_*)
 \end{pmatrix}
 \right)
$$
$K(X,X)_{i,j} = k(X_i, X_j)$  
Bedingte Verteilung:
$$
f(X_*)|f(X), X, X_* \sim \mathcal{N}(\mu, \Sigma)
$$
$$
  \mu = K(X,X_*) K(X,X)^{-1} f(X)
$$
$$
 \Sigma = K(X_*, X_*) - K(X_*, X) K(X, X)^{-1} K(X, X_*)
$$

## predict Algorithmus
Annahme: $y_i = f(x_i) + \varepsilon_i$, $\varepsilon_i \overset{\text{iid}}{\sim} \mathcal{N}(0, \sigma_n^2)$

Inputs: $X$ (inputs), $y$ (targets), $\sigma_n^2$ (noise),  
$K$ (covariance funtion), $X_*$ (test input)

\# 1 &nbsp; $L=\text{cholesky}(K(X,X)+\sigma_n^2 I)$  
\# 2 &nbsp;   $\alpha=\text{solve}(L^\top,\text{solve}(L,y))$  
\# 3 &nbsp;   $\bar{f}(X_*)=K(X,X_*)^\top\cdot\alpha$  
\# 4 &nbsp;   $v=\text{solve}(L,K(X,X_*))$  
\# 5 &nbsp;   $\overline{\text{V}}(\bar{f}(X_*))=K(X_*,X_*)-v^\top v$  

return: $\bar{f}(X_*)$, $\overline{\text{V}}(\bar{f}(X_*))$

## predict Algorithmus
Annahme: $y_i = f(x_i) + \varepsilon_i$, $\varepsilon_i \overset{\text{iid}}{\sim} \mathcal{N}(0, \sigma_n^2)$

Inputs: $X$ (inputs), $y$ (targets), $\sigma_n^2$ (noise),  
$K$ (covariance funtion), $X_*$ (test input)

\# 1 &nbsp; $\textcolor{red}{L =\text{cholesky}(K(X,X)+\sigma_n^2 I)}$  
\# 2 &nbsp;   $\textcolor{red}{\alpha =\text{solve}(L^\top,\text{solve}(L,y))}$  
\# 3 &nbsp;   $\bar{f}(X_*)=K(X,X_*)^\top\cdot\alpha$  
\# 4 &nbsp;   $v=\text{solve}(L,K(X,X_*))$  
\# 5 &nbsp;   $\overline{\text{V}}(\bar{f}(X_*))=K(X_*,X_*)-v^\top v$  

return: $\bar{f}(X_*)$, $\overline{\text{V}}(\bar{f}(X_*))$

## GPR
`GPR$new(X, y, noise, cov_func)`

- R6 Klasse
- Methoden 
    + `$predict`
    + `$plot`
    + `$plot_posterior_draws`
    + `$plot_posterior_variance`
- Unterklassen für häufig auftretende Kovarianzfunktionen, die lediglich Parametereingabe erfordern



## Regression
```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
set.seed(0)
X <- seq(-8, 8, by = 0.4)
y <- (X^3 + 2*sin(3*X))*exp(-abs(X)) + rnorm(length(X), sd = 0.1)
g <- ggplot2::ggplot(data = data.frame(x = X, y = y), ggplot2::aes(x = x, y = y, shape = 4)) +
                    ggplot2::scale_shape_identity() + ggplot2::theme_classic() + ggplot2::geom_point()
g + ggplot2::coord_cartesian(ylim = c(-2.2, 2.2), xlim = c(-10, 10))
```

## Regression

```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE}
set.seed(0)
X <- seq(-8, 8, by = 0.4)
y <- (X^3 + 2*sin(3*X))*exp(-abs(X)) + rnorm(length(X), sd = 0.1)
Gaussian <- GPR.sqrexp$new(X, y, noise = 0.01, l = 0.5)
g <- Gaussian$plot()$plot
g + ggplot2::coord_cartesian(ylim = c(-2.2, 2.2), xlim = c(-10, 10)) + ggplot2::scale_y_continuous("y")
```


## Optimierung der Hyperparameter

Suchen Kovarianzfunktion, die beobachtete Daten am besten erklärt

Methode: Maximiere Log-Likelihood der beobachteten Daten nach der Kovarianzfunktion $k$

## fit()

`fit(X, y, noise, cov_names)`

- Gibt die beste Kovarianzfunktion mit optimalen Parametern zurück
- Nutzen Newton-Methode, `optim()`
- Error handling bei numerischen Problemen der Cholesky Zerlegung


## Gewählte Kovarianzfunktion

```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=TRUE}
X <- matrix(seq(-5,5,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
Gaussian <- GPR.sqrexp$new(X, y, noise, 1)
Gaussian$plot()$plot
message("The chosen covariance function is sqrexp")
```

## Optimierte Kovarianzfunktion

```{r dpi=k, out.width="95%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE}
X <- matrix(seq(-5,5,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
z <- fit(X,y,noise,list("linear","constant","polynomial","sqrexp","gammaexp","rationalquadratic"))
Gaussian <- GPR$new(X, y, noise, z$func)
Gaussian$plot()$plot
```
```{r message=TRUE}
message(sprintf("The optimal covariance function is %s",z$cov))
```


## GP Classification
`GPC$new(X, y, cov_fun, epsilon)`

- R6 Klasse
- Methoden `$predict_class`, `$plot`
- Effizienz durch Vektorisierung

## Klassifikation

```{r dpi=k, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
set.seed(0)
g <- function(x){
    if (x == 0) return(-1)
    if (x == -3) return(-1)
    return(x)
  }
  f <- function(x) {
    g(((x[1] > -3.2 && x[1] < 1.8 && x[2] <= 3.2 && x[2] > 0) || (x[1] > -3 && x[1] < 3.5 && x[2] <= 3 && x[2] <= 0) ) - 
        (!((x[1] > -3.2 && x[1] < 1.8 && x[2] <= 3.2 && x[2] > 0) || (x[1] > -3 && x[1] < 3.5 && x[2] <= 3 && x[2] <= 0) )) -
        2*(x[1] > -1.5 && x[1] < 0 && x[2] < -1) -
        2*(x[1] > 0 && x[1] < 2 && x[2]  < -1 && sum(x) < -1.8) -
        2*(x[1] > -2 && x[1] < 0.5 && x[2] > 0 && x[2] < 2) -
        2*(x[1] > 1 && x[2] < 0 && x[2] > -3.5 && x[1] < 3.5 && sum(x) > -0.4))
  }
  
  limits <- matrix(c(-4, 4, -4, 4), nrow = 2, byrow = TRUE)
  training_size <- 600
  training_points <- t(sapply(1:nrow(limits), 
                        function(i) runif(training_size, limits[i, 1], limits[i, 2])))
  y <- apply(training_points, 2, f)
ggplot2::ggplot(data = data.frame(x = training_points[1, ], y = training_points[2, ], label = y), ggplot2::aes(x = x, y = y, shape = factor(label))) + ggplot2::theme_classic() +
 ggplot2::scale_y_continuous(expression(x[2])) +
 ggplot2::scale_x_continuous(expression(x[1])) + ggplot2::geom_point() + 
  ggplot2::scale_shape_manual(values = c(4, 2)) + 
  ggplot2::guides(shape = ggplot2::guide_legend(title = "Testpoints"))
```

## Klassifikation

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
set.seed(0)
 # Example R
  g <- function(x){
    if (x == 0) return(-1)
    if (x == -3) return(-1)
    return(x)
  }
  f <- function(x) {
    g(((x[1] > -3.2 && x[1] < 1.8 && x[2] <= 3.2 && x[2] > 0) || (x[1] > -3 && x[1] < 3.5 && x[2] <= 3 && x[2] <= 0) ) - 
        (!((x[1] > -3.2 && x[1] < 1.8 && x[2] <= 3.2 && x[2] > 0) || (x[1] > -3 && x[1] < 3.5 && x[2] <= 3 && x[2] <= 0) )) -
        2*(x[1] > -1.5 && x[1] < 0 && x[2] < -1) -
        2*(x[1] > 0 && x[1] < 2 && x[2]  < -1 && sum(x) < -1.8) -
        2*(x[1] > -2 && x[1] < 0.5 && x[2] > 0 && x[2] < 2) -
        2*(x[1] > 1 && x[2] < 0 && x[2] > -3.5 && x[1] < 3.5 && sum(x) > -0.4))
  }
  
  limits <- matrix(c(-4, 4, -4, 4), nrow = 2, byrow = TRUE)
  k <- function(x, y) sqrexp(x, y, 1)
  s <- simulate_classification(func = f, limits = limits, k = k, training_size = 600)
```

## Simulation
Framework, um Qualität von GP Methoden für verschiedene Daten zu testen

- `simulate_classification(func, limits, training_size)`

- `simulate_regression(func, limits, training_size)`

- `simulate_regression_gp(actual_cov, limits, training_size)`


## Simulation Beispiel

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
set.seed(0)
s <- simulate_regression_gp(cov_func(sqrexp, l = 1), limits = matrix(c(-5, 5), nrow = 1), 
                         training_size = 20, random_training = TRUE)
```



## Shiny App
