---
title: "Gaussian Processes"
author: "Michael Budjan, Moritz Haas, Konstantin Klumpp, Tim Reitze"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{bbm}
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Gaussian Processes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(gaussianprocess)
```
> "You shouldn't feel ashamed about your code - if it solves the problem, it's perfect just the way it is. But also, it could always be better." —- @hadleywickham
 [via](https://twitter.com/allimoberger/status/1085268564821585921)

This package implements
- gaussian process regression
- gaussian process binary classification
- clear and complete plots of the results
- optimization for a certain set of covariance functions.


A Gaussian Process is a stochastic process $(X_t)_{t \in T}$ with $\mathbb{E}\left[ X_t \right] = 0$ and for all finite subsets $S \subset T$, $(X_t)_{t \in S}$ has a multivariate normal distribution.

## Regression
Initialize an object of class GPR with training data X, target vector y, the vectorized covariance function `cov_func` and the noise level which is assumed.
```{r, eval=FALSE}
Gaussian <- GPR$new(X, y, cov_func, noise)
```
The following covariance functions have already been implemented and can be used by selecting the corresponding subclass:
- constant, $k(x,y) = c$,
- linear, $k(x,y) = \sum_{d=1}^D \sigma_d * x_d * y_d$,
- polynomial, $k(x,y) = (x * y + \sigma)^p$,
- squared exponential, $k(x,y) = exp(-\frac{|x-y|^2}{2 l^2})$,
- gamma exponential, $k(x,y) = exp(-(|x-y|/l)^\gamma)$,
- rational quadratic, $k(x,y) = \left(1+ \frac{|x-y|^2}{2 \alpha l^2}\right)^{-\alpha}$.

```{r, eval=FALSE}
Gaussian <- GPR.sqrexp$new(X, y, l = 1, noise = 0.5)
```
The class provides two methods, the first one is for the prediction of points

> “Prediction is very difficult, especially about the future. – Niels Bohr" -- Hadley Wickham
[via](https://www.quora.com/profile/Hadley-Wickham)

```{r, eval=FALSE}
Gaussian$predict(x, pointwise_var = FALSE)
```

> "Visualization can surprise you, but it doesn’t scale well. Modelling scales well, but it can’t surprise you." --Hadley Wickham
[via](https://www.johndcook.com/blog/2013/02/07/visualization-modeling-and-surprises/)

```{r, eval=FALSE}
Gaussian$plot(seq(-10, 10, by = 0.05))
```

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62}
X <- matrix(seq(-5,5,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
Gaussian <- GPR.sqrexp$new(X, y, 1, noise)
Gaussian$plot(seq(-10,10, by = 0.05))
```


```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62}
X <- matrix(seq(-3,3,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
Gaussian <- GPR.gammaexp$new(X, y, 1, 1.5, noise = 0.1)
Gaussian$plot_posterior_draws(10, seq(-3,3, by = 0.1))
```

## Optimization of Hyperparameters
> "Any real data analysis involves data manipulation (sometimes called wrangling or munging), visualization and modelling." -- Hadley Wickham
[via](http://bulletin.imstat.org/2014/09/data-science-how-is-it-different-to-statistics%E2%80%89/)


```{r, eval=FALSE}
fit(X, y, noise = 1, cov_names = list("sqrexp","rationalquadratic"))
```

> "So I have experienced this many times I look at my code and it's not only like a stranger has written it, but a potentially insane stranger [...] and I have no idea what it does."-- Hadley Wickham
[via](https://youtu.be/rz3_FDVt9eg?t=583)

```{r, eval=TRUE, include=FALSE}
X <- matrix(seq(-5,5,by = 0.2), nrow = 1)
y <- c(0.1*X^3 + rnorm(length(X), 0, 1))

z <- fit(X, y, noise = 1, cov_names = list("sqrexp","rationalquadratic"))

print(z)
#Gaussian <- GPR$new(X, y, function(x,y) do.call(cov_df[z$cov, ]$func[[1]], append(list(x, y), z$par)), noise = 1)
#Gaussian$plot(seq(-5, 5, by = 0.1))
```


## Classification

> "Dear past-Hadley: PLEASE COMMENT YOUR CODE BETTER. Love present-Hadley" -- @hadleywickham
[via](https://twitter.com/hadleywickham/status/718203628528349184)

```{r, eval=FALSE}
Gaussian_classifier <- GPC$new(X, y, cov_func, noise = 0.5)
```


```{r, eval=FALSE}
Gaussian_classifier$plot(seq(-2, 2, by = 0.1))
```
> "The fact that data science exists as a field is a colossal failure of statistics. To me, that is what statistics is all about." -- Hadley Wickham
[via](https://priceonomics.com/hadley-wickham-the-man-who-revolutionized-r/)

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE}
X <- matrix(seq(-1,1,by = 0.1), nrow = 1)
y <- 2*as.integer(X > 0) - 1
kappa <- function(x,y) exp(-3*(x - y)^2)
Gaussian_classifier <- GPC$new(X, y, kappa, 1e-5)
Gaussian_classifier$plot(seq(-2,2, by = 0.1))
```

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE}
#Copy function definitions, since they aren't being exported by the package
multivariate_normal <- function(n, mean, covariance) {
  stopifnot(is.numeric(mean), is.numeric(covariance), length(mean) == nrow(covariance))
  L <- t(chol(covariance))
  c(mean) + L %*% matrix(rnorm(n*length(mean), 0, 1), nrow = length(mean))
}
sqrexp <- function(x, y, l) UseMethod("sqrexp")
sqrexp.matrix <- function(x, y, l) exp(-colSums((x - y)^2)/(2 * l^2))
sqrexp.numeric <- function(x, y, l) exp(-sum((x - y)^2)/(2 * l^2))

s <- seq(-1, 1, by = 0.5)
X <- matrix(c(rep(s, each = length(s)), rep(s, times = length(s))), nrow = 2, byrow = T)
y <- 2*as.integer(X[1, ] > X[2, ]) - 1
kappa <- function(x,y) sqrexp(x,y,l=1)
gaussian_classifier <- GPC$new(X, y, kappa, 1e-5)
s <- seq(-1, 1, by = 0.1)
testpoints <- matrix(c(rep(s, each = length(s)), rep(s, times = length(s))), nrow = 2, byrow = T)
gaussian_classifier$plot(testpoints)
```

TEXT

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=FALSE}
n <- 10
X <- cbind(multivariate_normal(n, c(0.5,0.5), diag(c(0.1,0.1))), multivariate_normal(n, c(-0.5,-0.5), diag(c(0.1,0.1))))
y <- rep(c(1,-1), each = n)
kappa <- function(x,y) sqrexp(x,y,l=1)
gaussian_classifier <- GPC$new(X, y, kappa, 1e-5)
s <- seq(-1, 1, by = 0.1)
testpoints <- matrix(c(rep(s, each = length(s)), rep(s, times = length(s))), nrow = 2, byrow = T)
gaussian_classifier$plot(testpoints)
```
