---
title: "Gaussian Processes"
author: "Michael Budjan, Moritz Haas, Konstantin Klumpp, Tim Reitze"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{bbm}
   - \usepackage{amssymb}
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Gaussian Processes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(gaussianprocess)
```

$\newcommand\erw[1]{\mathbb{E}\left[#1\right]}
\newcommand\Var[1]{\text{Var}\left( #1 \right)}
\newcommand\pos[1]{\mathbb{P}\left(#1\right)}
\newcommand\norm[1]{\mathcal{N}\left(#1\right)}
\newcommand\weakcon{\overset{\mathcal{D}}{\longrightarrow}}
\newcommand\stochcon{\overset{\mathds{P}}{\longrightarrow}}$


> "You shouldn't feel ashamed about your code - if it solves the problem, it's perfect just the way it is. But also, it could always be better." —- @hadleywickham
 [via](https://twitter.com/allimoberger/status/1085268564821585921)

This package implements

- Gaussian process regression
- Gaussian process binary classification
- clear and complete plots of the results
- optimization for a certain set of covariance functions.

A Gaussian Process is a stochastic process $(f_x)_{x \in X}$ where every finite subset $Y \subset X$, $(f_x)_{x \in Y}$ is multivariate normally distributed. 
With the notation $f(x) := f_x$ it is more intuitive that $(f_x)_{x \in X}$ can be seen
as a random function $f:X \to \mathbb{R}$.
In the following $X$ will be a subset of $\mathbb{R}^D$ for some $D \geq 1$.
The Gaussian Process $f(x)$ is uniquely specified by its mean function $m(x)=\erw{f(x)}$ and covariance function $k(x,x')=\text{Cov}(f(x),f(x'))$. <br/>
Now one observes data sets $\{(x_i,y_i)| x_i \in X, y_i\in\mathbb{R}, i \leq n\}$ which we model as $$y_i=f(x_i)+\varepsilon_i,$$ typically assuming $\varepsilon \sim \norm{0,\sigma_n^2}$, $\sigma_n^2=\sigma^2\cdot I_n$ and $m(x)=\erw{f(x)} = 0$.
Given such a data set, one would like to do two things:

- Predict the value $f(x)$ of the Gaussian Process at other points $x\in X$ based on the given observations (see section Regression). And to do so:
- Find the covariance function $k$ for which the given observations $(x_i,y_i)$ are most likely (see section Optimization of Hyperparameters).





## Regression
Initialize an object of class GPR with training data $X$, target vector $y$, the vectorized covariance function `cov_func` and the assumed noise level $\sigma$.
```{r, eval=FALSE}
Gaussian <- GPR$new(X, y, cov_func, noise)
```
The following covariance functions have already been implemented and can be used by selecting the corresponding subclass:

- constant, $k(x,y) = c$,

- linear, $k(x,y) = \sum\limits_{d=1}^D \sigma_d \cdot x_d \cdot y_d$,

- polynomial, $k(x,y) = (x \cdot y + \sigma)^p$,

- sqrexp, $k(x,y) = \exp\left(-\frac{|x-y|^2}{2 l^2}\right)$,
- gammaexp, $k(x,y) = \exp \left(-\left( \frac{|x-y|}{l} \right)^\gamma \right)$,
- rationalquadratic, $k(x,y) = \left(1+ \frac{|x-y|^2}{2 \alpha l^2}\right)^{-\alpha}$.

```{r, eval=FALSE}
Gaussian <- GPR.sqrexp$new(X, y, l = 1, noise = 0.5)
```


The class provides two methods, the first one is for the prediction of points:

> “Prediction is very difficult, especially about the future. – Niels Bohr" -- Hadley Wickham
[via](https://www.quora.com/profile/Hadley-Wickham)

```{r, eval=FALSE}
Gaussian$predict(X_star, pointwise_var = TRUE)
```

`$predict` returns a matrix of the expected value of the underlying function f and its variance for the test points. If the input is a vector of length n, predict will interpret it as n test points.
For `pointwise_var = FALSE` , predict will return the predicted covariance matrix `cov(X_star, X_star)` instead of only its diagonal.



```{r, eval=FALSE}
Gaussian$plot(seq(-10, 10, by = 0.05))
```

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62}
X <- matrix(seq(-5,5,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
Gaussian <- GPR.sqrexp$new(X, y, 1, noise)
Gaussian$plot(seq(-10,10, by = 0.05))
```


```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62}
X <- matrix(seq(-3,3,by = 0.5), nrow = 1)
noise <- 0.5
y <- c(0.1*X^3 + rnorm(length(X), 0, noise))
Gaussian <- GPR.gammaexp$new(X, y, 1, 1.5, noise = 0.1)
Gaussian$plot_posterior_draws(10, seq(-3,3, by = 0.1))
```

## Optimization of Hyperparameters
> "Any real data analysis involves data manipulation (sometimes called wrangling or munging), visualization and modelling." -- Hadley Wickham
[via](http://bulletin.imstat.org/2014/09/data-science-how-is-it-different-to-statistics%E2%80%89/)

For a given set of data points the `fit()` function can select the optimal hyperparameters of the covariance function and choose the best covariance function from a given list. `X` and `y` have to be data from a regression problem with assumed `noise` and `cov_names` is a list of names of covariance functions. Then the maximum of the marginal log likelihood (given by the formula from the `$predict` function) is calculated over the parameter region. 
$$ \log(y| X, \theta) = - \frac{1}{2} y^\top K^{-1} y - \frac{1}{2} \log |K| - \frac{n}{2}\log 2 \pi $$
This is done by using the derivative of the likelihood and the `optim` function.
$$ \frac{\partial}{\partial \theta_j} \log(y| X, \theta) = \frac{1}{2} \text{tr} \left( (\alpha \alpha^\top - K^{-1}) \frac{\partial K}{\partial \theta_j} \right) \; \text{ where } \alpha = K^{-1}y$$
```{r, eval=FALSE}
fit(X, y, noise, cov_names)
```

> "So I have experienced this many times I look at my code and it's not only like a stranger has written it, but a potentially insane stranger [...] and I have no idea what it does."-- Hadley Wickham
[via](https://youtu.be/rz3_FDVt9eg?t=583)

```{r, eval=TRUE, include=TRUE}
X <- matrix(seq(-5,5,by = 0.2), nrow = 1)
y <- c(0.1*X^3 + rnorm(length(X), 0, 1))

z <- fit(X, y, noise = 1, cov_names = list("sqrexp","rationalquadratic"))

print(z)
#Gaussian <- GPR$new(X, y, function(x,y) do.call(cov_df[z$cov, ]$func[[1]], append(list(x, y), z$par)), noise = 1)
#Gaussian$plot(seq(-5, 5, by = 0.1))
```


## Classification

The class `GPC` solves binary classification problems by using a Gaussian process. 
> "Dear past-Hadley: PLEASE COMMENT YOUR CODE BETTER. Love present-Hadley" -- @hadleywickham
[via](https://twitter.com/hadleywickham/status/718203628528349184)

```{r, eval=FALSE}
Gaussian_classifier <- GPC$new(X, y, cov_func, noise)
```

The `$predict` function uses the Laplace approximation
> "The fact that data science exists as a field is a colossal failure of statistics. To me, that is what statistics is all about." -- Hadley Wickham
[via](https://priceonomics.com/hadley-wickham-the-man-who-revolutionized-r/)

The `$plot` function displays the results of the classification.

- For a set of one dimensional test points the decision function and additionally the decision regions and the training points are plotted
```{r, eval=FALSE}
Gaussian_classifier$plot(seq(-2, 2, by = 0.1))
```

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE}
X <- matrix(seq(-1,1,by = 0.1), nrow = 1)
y <- 2*as.integer(X > 0) - 1
kappa <- function(x,y) exp(-3*(x - y)^2)
Gaussian_classifier <- GPC$new(X, y, kappa, 1e-5)
Gaussian_classifier$plot(seq(-2,2, by = 0.1))
```

- For two dimensional data the test points and the decision regions are shown.

```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE}
#Copy function definitions, since they aren't being exported by the package
multivariate_normal <- function(n, mean, covariance) {
  stopifnot(is.numeric(mean), is.numeric(covariance), length(mean) == nrow(covariance))
  L <- t(chol(covariance))
  c(mean) + L %*% matrix(rnorm(n*length(mean), 0, 1), nrow = length(mean))
}
sqrexp <- function(x, y, l) UseMethod("sqrexp")
sqrexp.matrix <- function(x, y, l) exp(-colSums((x - y)^2)/(2 * l^2))
sqrexp.numeric <- function(x, y, l) exp(-sum((x - y)^2)/(2 * l^2))

s <- seq(-1, 1, by = 0.5)
X <- matrix(c(rep(s, each = length(s)), rep(s, times = length(s))), nrow = 2, byrow = T)
y <- 2*as.integer(X[1, ] > X[2, ]) - 1
kappa <- function(x,y) sqrexp(x,y,l=1)
gaussian_classifier <- GPC$new(X, y, kappa, 1e-5)
s <- seq(-1, 1, by = 0.1)
testpoints <- matrix(c(rep(s, each = length(s)), rep(s, times = length(s))), nrow = 2, byrow = T)
gaussian_classifier$plot(testpoints)
```


```{r dpi=1000, out.width="100%", echo=FALSE, fig.width = 5, fig.asp = .62, message=FALSE, eval=TRUE}
n <- 10
X <- cbind(multivariate_normal(n, c(0.5,0.5), diag(c(0.1,0.1))), multivariate_normal(n, c(-0.5,-0.5), diag(c(0.1,0.1))))
y <- rep(c(1,-1), each = n)
kappa <- function(x,y) sqrexp(x,y,l=1)
gaussian_classifier <- GPC$new(X, y, kappa, 1e-5)
s <- seq(-1, 1, by = 0.1)
testpoints <- matrix(c(rep(s, each = length(s)), rep(s, times = length(s))), nrow = 2, byrow = T)
gaussian_classifier$plot(testpoints)
```

## Simulation

